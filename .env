# LangCoach Environment Configuration
#
# 使用说明 / Usage:
# 1. 复制此文件到 .env: cp .env.example .env
# 2. 编辑 .env 文件，填写您的实际配置值
# 3. 应用会自动加载 .env 文件中的配置（无需手动 export）
# 4. 运行 `python test_env_loading.py` 验证配置是否正确加载
#
# Copy this file to .env and fill in your actual values
# The application will automatically load .env file on startup

# ============================================
# Phase 2: Long-Term Memory Configuration
# ============================================

# Milvus Vector Database (Phase 2 新增)
MILVUS_HOST=milvus                 # Docker 内使用 "milvus"，本地开发使用 "localhost"
MILVUS_PORT=19530                  # Milvus 默认端口

# ============================================
# LLM Provider Configuration
# ============================================
# 重要：所有 LLM 配置都在此 .env 文件中设置
# 代码中不包含硬编码的默认值

# 提供者优先级
# 格式：逗号分隔的提供者列表，从左到右优先级递减
# 默认：ollama,deepseek,openai
LLM_PROVIDER_PRIORITY=deepseek,ollama,openai

# ---------------------------------------------
# 选项 1: Ollama (本地部署 - 默认首选)
# ---------------------------------------------
# 优势：免费、隐私、无需 API key
# 必须配置 OLLAMA_MODEL 和 OLLAMA_BASE_URL 才能启用

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=hf.co/unsloth/GLM-4-9B-0414-GGUF:Q8_K_XL
OLLAMA_TEMPERATURE=0.8
OLLAMA_MAX_TOKENS=8192
# OLLAMA_ENABLED=true              # 默认启用，设为 false 可禁用

# 安装 Ollama:
# 1. 访问 https://ollama.com 下载安装
# 2. 运行: ollama serve
# 3. 拉取模型: ollama pull unsloth/GLM-4-9B-0414-GGUF:Q8_K_XL
# 4. 验证: ollama list

# 其他推荐模型：
# - llama3.1:8b-instruct-q8_0  (Llama 3.1 8B)
# - qwen2.5:14b                (Qwen 2.5 14B)
# - mistral:7b-instruct-v0.3   (Mistral 7B)

# ---------------------------------------------
# 选项 2: DeepSeek API (高性价比)
# ---------------------------------------------
# 优势：便宜、性能好、支持中文
# 官网：https://platform.deepseek.com
# 必须配置有效的 DEEPSEEK_API_KEY 才能启用

DEEPSEEK_API_KEY=sk-87e02a9c45174f338e799bcb13559798
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_BASE_URL=https://api.deepseek.com
DEEPSEEK_TEMPERATURE=0.8
DEEPSEEK_MAX_TOKENS=8192
# DEEPSEEK_ENABLED=true            # 默认启用，设为 false 可禁用

# ---------------------------------------------
# 选项 3: OpenAI API
# ---------------------------------------------
# 优势：性能强、稳定性好
# 官网：https://platform.openai.com
# 必须配置有效的 OPENAI_API_KEY 才能启用

OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini
# OPENAI_BASE_URL=                 # 可选：用于兼容的 API（如 Azure OpenAI）
OPENAI_TEMPERATURE=0.8
OPENAI_MAX_TOKENS=8192
# OPENAI_ENABLED=true              # 默认启用，设为 false 可禁用

# 推荐模型：
# - gpt-4o-mini    (便宜、快速)
# - gpt-4o         (性能最佳)
# - gpt-4-turbo    (长上下文)
# - gpt-3.5-turbo  (最便宜)

# ============================================
# Application Configuration
# ============================================

# =============================================================================
# MINI PROGRAM API CONFIGURATION
# =============================================================================

# Ollama/LLM Settings (for Mini Program API)
OLLAMA_TIMEOUT=5
OLLAMA_NUM_PREDICT=512

# STT Settings  
STT_MODEL=unsloth/whisper-large-v3
STT_SAMPLE_RATE=16000

# TTS Settings
TTS_DEFAULT_SPEAKER=Ceylia
TTS_SAMPLE_RATE=24000
TTS_FORMAT=mp3

# API Settings
API_HOST=0.0.0.0
API_PORT=8600
CORS_ORIGINS=*

# Session Management
MAX_RECENT_MESSAGES=6
MAX_REPLY_LENGTH=300
MAX_REPLY_SENTENCES=200
AUDIO_CACHE_HOURS=1

# Available scenarios (comma-separated)
AVAILABLE_SCENARIOS=job_interview,hotel_checkin,renting,salary_negotiation

# Prompts directory (relative path)
PROMPTS_DIR=prompts

# Gradio Web Server
GRADIO_PORT=8300                   # Phase 2 默认端口改为 8300
GRADIO_FORCE_RESTART=false         # 设置为 true 自动停止占用端口的进程

# Python Configuration
PYTHONUNBUFFERED=1                 # 禁用 Python 输出缓冲

# ============================================
# Docker Volume Directory (可选)
# ============================================

# 用于存储 Milvus/etcd/minio 数据
# DOCKER_VOLUME_DIRECTORY=./volumes

# ============================================
# Memory System Configuration (高级)
# ============================================

# 使用哪个嵌入模型（如果同时配置了 OpenAI 和 Ollama）
# 默认：优先使用 OpenAI (更准确)
# USE_OLLAMA_EMBEDDINGS=false

# 上下文窗口限制（单位：tokens）
# MAX_CONTEXT_TOKENS=3000

# ============================================
# Development Configuration
# ============================================

# 日志级别 (DEBUG, INFO, WARNING, ERROR)
# LOG_LEVEL=INFO

# 是否启用详细调试日志
# DEBUG=false

# ============================================
# 快速开始指南
# ============================================

# 推荐方案 1：使用 Ollama（完全免费，本地运行）
# 1. 安装 Ollama: https://ollama.com
# 2. 启动服务: ollama serve
# 3. 下载模型: ollama pull unsloth/GLM-4-9B-0414-GGUF:Q8_K_XL
# 4. 启动应用: docker-compose up -d
# 5. 访问: http://localhost:8300

# 推荐方案 2：使用 DeepSeek（低成本 API）
# 1. 注册账号: https://platform.deepseek.com
# 2. 获取 API key
# 3. 设置环境变量: DEEPSEEK_API_KEY=your_key
# 4. 启动应用: docker-compose up -d
# 5. 访问: http://localhost:8300

# 推荐方案 3：使用 OpenAI（高性能）
# 1. 注册账号: https://platform.openai.com
# 2. 获取 API key
# 3. 设置环境变量: OPENAI_API_KEY=your_key
# 4. 启动应用: docker-compose up -d
# 5. 访问: http://localhost:8300

# ============================================
# Speech API Configuration
# ============================================

# Speech API 服务地址（STT/TTS）
SPEECH_API_URL=http://localhost:8600
SPEECH_API_HOST=0.0.0.0
SPEECH_API_PORT=8600

# 是否在启动时预加载模型
# 是否在启动时预加载模型
PRELOAD_MODELS=true

# ============================================
# Evaluation Framework Configuration
# ============================================

# 评估输出目录
EVAL_OUTPUT_DIR=evaluation/reports

# 默认评估样本数（设为空使用全部100条）
# EVAL_DEFAULT_SAMPLES=

# E2E 延迟目标（毫秒）
EVAL_TARGET_LATENCY_MS=3000

# TTS 评估模式 (fast=Edge-TTS, local=Orpheus)
EVAL_TTS_MODE=fast

# 评估报告格式（逗号分隔：json,md,html,txt）
EVAL_REPORT_FORMATS=json,md,html