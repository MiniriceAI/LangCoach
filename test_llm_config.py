#!/usr/bin/env python3
"""
æµ‹è¯• LLM é…ç½®å’Œå·¥å‚åŠŸèƒ½
éªŒè¯æ‰€æœ‰æä¾›è€…çš„é…ç½®å’Œåˆ›å»ºæµç¨‹
"""

import os
import sys

# ç¡®ä¿å¯ä»¥å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.agents.llm_config import get_llm_config, reload_llm_config
from src.agents.llm_factory import (
    create_llm,
    list_available_providers,
    get_current_provider_info
)
from src.utils.logger import LOG


def print_section(title: str):
    """æ‰“å°åˆ†æ®µæ ‡é¢˜"""
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)


def test_llm_config():
    """æµ‹è¯• LLM é…ç½®åŠ è½½"""
    print_section("æµ‹è¯• 1: LLM é…ç½®åŠ è½½")

    config = get_llm_config()

    print(f"\nğŸ“‹ é…ç½®ä¼˜å…ˆçº§: {' > '.join(config.priority)}")
    print(f"ğŸ“‹ å¯ç”¨æä¾›è€…: {', '.join(config.list_available_providers())}")

    print("\nğŸ“ è¯¦ç»†é…ç½®:")
    for provider_name in config.priority:
        provider_config = config.get_provider_config(provider_name)
        if provider_config:
            print(f"\n  âœ… {provider_name.upper()}:")
            print(f"     æ¨¡å‹: {provider_config.model}")
            print(f"     åœ°å€: {provider_config.base_url}")
            print(f"     æ¸©åº¦: {provider_config.temperature}")
            print(f"     Max Tokens: {provider_config.max_tokens}")
            if provider_config.api_key:
                masked_key = provider_config.api_key[:8] + "..." + provider_config.api_key[-4:]
                print(f"     API Key: {masked_key}")
        else:
            print(f"\n  âš ï¸  {provider_name.upper()}: æœªé…ç½®æˆ–å·²ç¦ç”¨")


def test_list_available_providers():
    """æµ‹è¯•åˆ—å‡ºå¯ç”¨æä¾›è€…"""
    print_section("æµ‹è¯• 2: åˆ—å‡ºå¯ç”¨æä¾›è€…")

    providers = list_available_providers()
    print(f"\nå¯ç”¨æä¾›è€…æ•°é‡: {len(providers)}")

    if providers:
        print("\næä¾›è€…åˆ—è¡¨:")
        for i, provider in enumerate(providers, 1):
            print(f"  {i}. {provider}")
    else:
        print("\nâš ï¸  æ²¡æœ‰å¯ç”¨çš„æä¾›è€…")
        print("è¯·è‡³å°‘é…ç½®ä»¥ä¸‹ä¹‹ä¸€:")
        print("  - Ollama (é»˜è®¤ï¼Œæ— éœ€ API key)")
        print("  - DeepSeek (è®¾ç½® DEEPSEEK_API_KEY)")
        print("  - OpenAI (è®¾ç½® OPENAI_API_KEY)")


def test_get_current_provider_info():
    """æµ‹è¯•è·å–å½“å‰æä¾›è€…ä¿¡æ¯"""
    print_section("æµ‹è¯• 3: è·å–å½“å‰æä¾›è€…ä¿¡æ¯")

    info = get_current_provider_info()

    if info["available"]:
        print(f"\nâœ… å½“å‰æä¾›è€…: {info['provider'].upper()}")
        print(f"   æ¨¡å‹: {info['model']}")
        print(f"   åœ°å€: {info['base_url']}")
        print(f"   æ¸©åº¦: {info['temperature']}")
        print(f"   Max Tokens: {info['max_tokens']}")
    else:
        print("\nâŒ æ²¡æœ‰å¯ç”¨çš„æä¾›è€…")


def test_create_llm_auto():
    """æµ‹è¯•è‡ªåŠ¨åˆ›å»º LLMï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰"""
    print_section("æµ‹è¯• 4: è‡ªåŠ¨åˆ›å»º LLMï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰")

    try:
        llm = create_llm()
        print(f"\nâœ… æˆåŠŸåˆ›å»º LLM å®ä¾‹")
        print(f"   ç±»å‹: {type(llm).__name__}")
        print(f"   æ¨¡å‹: {getattr(llm, 'model', 'N/A')}")

        # æµ‹è¯•ç®€å•è°ƒç”¨
        print("\nğŸ§ª æµ‹è¯•ç®€å•å¯¹è¯...")
        response = llm.invoke("Say 'Hello' in one word")
        print(f"   å›å¤: {response.content[:100]}...")

    except Exception as e:
        print(f"\nâŒ åˆ›å»º LLM å¤±è´¥: {str(e)}")


def test_create_llm_specific():
    """æµ‹è¯•åˆ›å»ºæŒ‡å®šæä¾›è€…çš„ LLM"""
    print_section("æµ‹è¯• 5: åˆ›å»ºæŒ‡å®šæä¾›è€…çš„ LLM")

    providers = list_available_providers()

    for provider_name in providers:
        print(f"\n  æµ‹è¯•åˆ›å»º {provider_name.upper()} LLM...")
        try:
            llm = create_llm(provider_name)
            print(f"  âœ… æˆåŠŸåˆ›å»º {provider_name} LLM")
            print(f"     ç±»å‹: {type(llm).__name__}")
        except Exception as e:
            print(f"  âŒ åˆ›å»ºå¤±è´¥: {str(e)[:100]}")


def test_priority_override():
    """æµ‹è¯•ä¼˜å…ˆçº§è¦†ç›–"""
    print_section("æµ‹è¯• 6: ä¼˜å…ˆçº§è¦†ç›–")

    print("\nğŸ“ åŸå§‹ä¼˜å…ˆçº§:")
    original_priority = os.getenv("LLM_PROVIDER_PRIORITY", "æœªè®¾ç½®")
    print(f"   LLM_PROVIDER_PRIORITY={original_priority}")

    config = get_llm_config()
    print(f"   å®é™…ä¼˜å…ˆçº§: {' > '.join(config.priority)}")

    print("\nğŸ’¡ æç¤º:")
    print("   å¯ä»¥é€šè¿‡è®¾ç½® LLM_PROVIDER_PRIORITY ç¯å¢ƒå˜é‡æ¥è¦†ç›–ä¼˜å…ˆçº§")
    print("   ä¾‹å¦‚: export LLM_PROVIDER_PRIORITY=deepseek,openai,ollama")


def test_provider_enable_disable():
    """æµ‹è¯•æä¾›è€…å¯ç”¨/ç¦ç”¨"""
    print_section("æµ‹è¯• 7: æä¾›è€…å¯ç”¨/ç¦ç”¨")

    providers = ["ollama", "deepseek", "openai"]

    print("\nğŸ“ å„æä¾›è€…å¯ç”¨çŠ¶æ€:")
    for provider in providers:
        env_var = f"{provider.upper()}_ENABLED"
        enabled = os.getenv(env_var, "true").lower() in ("true", "1", "yes", "on")
        status = "âœ… å¯ç”¨" if enabled else "âŒ ç¦ç”¨"
        print(f"   {provider.upper()}: {status} ({env_var}={os.getenv(env_var, 'true')})")

    print("\nğŸ’¡ æç¤º:")
    print("   å¯ä»¥é€šè¿‡è®¾ç½® {PROVIDER}_ENABLED=false æ¥ç¦ç”¨æŸä¸ªæä¾›è€…")
    print("   ä¾‹å¦‚: export OLLAMA_ENABLED=false")


def print_summary():
    """æ‰“å°æµ‹è¯•æ€»ç»“"""
    print_section("ğŸ¯ æµ‹è¯•æ€»ç»“")

    config = get_llm_config()
    info = get_current_provider_info()

    print(f"\nâœ… é…ç½®åŠ è½½: æˆåŠŸ")
    print(f"âœ… å¯ç”¨æä¾›è€…: {len(config.list_available_providers())} ä¸ª")

    if info["available"]:
        print(f"âœ… å½“å‰æä¾›è€…: {info['provider'].upper()} ({info['model']})")
        print(f"\nğŸ‰ LLM é…ç½®ç³»ç»Ÿæ­£å¸¸å·¥ä½œï¼")
    else:
        print(f"âš ï¸  å½“å‰æä¾›è€…: æ— ")
        print(f"\nâš ï¸  è¯·é…ç½®è‡³å°‘ä¸€ä¸ª LLM æä¾›è€…")


def print_env_config_help():
    """æ‰“å°ç¯å¢ƒå˜é‡é…ç½®å¸®åŠ©"""
    print_section("ğŸ’¡ ç¯å¢ƒå˜é‡é…ç½®å¸®åŠ©")

    print("\nğŸ“‹ æ ¸å¿ƒé…ç½®:")
    print("  LLM_PROVIDER_PRIORITY    # æä¾›è€…ä¼˜å…ˆçº§ï¼ˆé€—å·åˆ†éš”ï¼‰")
    print("")
    print("ğŸ“‹ Ollama é…ç½® (é»˜è®¤æä¾›è€…):")
    print("  OLLAMA_MODEL             # æ¨¡å‹åç§°")
    print("  OLLAMA_BASE_URL          # æœåŠ¡åœ°å€")
    print("  OLLAMA_TEMPERATURE       # æ¸©åº¦å‚æ•°")
    print("  OLLAMA_MAX_TOKENS        # æœ€å¤§ token æ•°")
    print("  OLLAMA_ENABLED           # æ˜¯å¦å¯ç”¨")
    print("")
    print("ğŸ“‹ DeepSeek é…ç½®:")
    print("  DEEPSEEK_API_KEY         # API å¯†é’¥ (å¿…éœ€)")
    print("  DEEPSEEK_MODEL           # æ¨¡å‹åç§°")
    print("  DEEPSEEK_BASE_URL        # API åœ°å€")
    print("  DEEPSEEK_TEMPERATURE     # æ¸©åº¦å‚æ•°")
    print("  DEEPSEEK_MAX_TOKENS      # æœ€å¤§ token æ•°")
    print("  DEEPSEEK_ENABLED         # æ˜¯å¦å¯ç”¨")
    print("")
    print("ğŸ“‹ OpenAI é…ç½®:")
    print("  OPENAI_API_KEY           # API å¯†é’¥ (å¿…éœ€)")
    print("  OPENAI_MODEL             # æ¨¡å‹åç§°")
    print("  OPENAI_BASE_URL          # API åœ°å€ (å¯é€‰)")
    print("  OPENAI_TEMPERATURE       # æ¸©åº¦å‚æ•°")
    print("  OPENAI_MAX_TOKENS        # æœ€å¤§ token æ•°")
    print("  OPENAI_ENABLED           # æ˜¯å¦å¯ç”¨")

    print("\nğŸ“– è¯¦ç»†æ–‡æ¡£: è¯·å‚è€ƒ .env.example æ–‡ä»¶")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "ğŸš€" * 35)
    print("  LangCoach - LLM é…ç½®ä¸å·¥å‚æµ‹è¯•")
    print("  Phase 2.5: å¢å¼ºçš„ LLM é…ç½®ç³»ç»Ÿ")
    print("ğŸš€" * 35)

    print("\nğŸ“ ç¯å¢ƒä¿¡æ¯:")
    print(f"  Python: {sys.version.split()[0]}")
    print(f"  å·¥ä½œç›®å½•: {os.getcwd()}")

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    try:
        test_llm_config()
        test_list_available_providers()
        test_get_current_provider_info()
        test_priority_override()
        test_provider_enable_disable()
        test_create_llm_auto()
        test_create_llm_specific()

        print_summary()
        print_env_config_help()

        print("\n" + "=" * 70)
        print("  âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
        print("=" * 70 + "\n")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
