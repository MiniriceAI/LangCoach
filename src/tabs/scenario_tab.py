# tabs/scenario_tab.py

import gradio as gr
import numpy as np
import io
import base64
import os
import requests
from typing import Optional, Tuple
from agents.scenario_agent import ScenarioAgent
from agents.conversation_config import (
    ConversationConfig,
    DifficultyLevel,
    TurnOption,
    create_config,
)
from utils.logger import LOG

# Speech API é…ç½®ï¼ˆé€šè¿‡ HTTP è°ƒç”¨ç‹¬ç«‹çš„ Speech API æœåŠ¡ï¼‰
SPEECH_API_URL = os.getenv("SPEECH_API_URL", "http://localhost:8600")

# æ”¯æŒçš„ Speaker åˆ—è¡¨
SPEAKER_CHOICES = [
    ("Ceylia", "Ceylia"),
    ("Tifa", "Tifa"),
]


def extract_english_response(bot_message: str) -> str:
    """
    ä» AI å›å¤ä¸­æå–çº¯è‹±æ–‡éƒ¨åˆ†ï¼ˆæ’é™¤å¯¹è¯æç¤ºï¼‰
    
    AI å›å¤æ ¼å¼é€šå¸¸æ˜¯:
    - è‹±æ–‡å›å¤
    - å¯¹è¯æç¤º:
    - è‹±æ–‡æç¤º
    - ä¸­æ–‡ç¿»è¯‘
    
    æˆ‘ä»¬åªéœ€è¦ç¬¬ä¸€éƒ¨åˆ†çš„è‹±æ–‡å›å¤ç”¨äº TTS
    """
    if not bot_message:
        return ""
    
    import re
    
    # ç¬¬ä¸€æ­¥ï¼šç§»é™¤ "LangCoach:" æˆ– "**LangCoach:**" å‰ç¼€
    text = bot_message.strip()
    prefix_patterns = [
        r'^\*\*LangCoach:\*\*\s*',
        r'^LangCoach:\s*',
        r'^\*\*LangCoachï¼š\*\*\s*',
        r'^LangCoachï¼š\s*',
    ]
    for prefix in prefix_patterns:
        text = re.sub(prefix, '', text)
    
    # ç¬¬äºŒæ­¥ï¼šæŒ‰ç…§"å¯¹è¯æç¤º"æˆ–"Dialogue hint"åˆ†å‰²ï¼Œåªå–ä¹‹å‰çš„éƒ¨åˆ†
    separators = [
        r'\n\n\*\*å¯¹è¯æç¤º[ï¼š:]\*\*',
        r'\n\nå¯¹è¯æç¤º[ï¼š:]',
        r'\nå¯¹è¯æç¤º[ï¼š:]',
        r'\n\nDialogue [Hh]int[ï¼š:]',
        r'\n\n\*\*å¯¹è¯æç¤º',
    ]
    
    for sep in separators:
        parts = re.split(sep, text, maxsplit=1)
        if len(parts) > 1:
            english_part = parts[0].strip()
            if english_part:
                return english_part
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†éš”ç¬¦ï¼Œè¿”å›æ•´ä¸ªæ¶ˆæ¯çš„ç¬¬ä¸€æ®µï¼ˆåˆ°åŒæ¢è¡Œç¬¦ä¸ºæ­¢ï¼‰
    paragraphs = text.split('\n\n')
    if paragraphs:
        return paragraphs[0].strip()
    
    return text.strip()


def synthesize_speech(text: str, speaker: str, fast_mode: bool = True) -> Optional[Tuple[int, np.ndarray]]:
    """
    é€šè¿‡ Speech API åˆæˆè¯­éŸ³

    Args:
        text: è¦åˆæˆçš„æ–‡æœ¬
        speaker: è¯´è¯äººï¼ˆCeylia æˆ– Tifaï¼‰
        fast_mode: ä½¿ç”¨Edge-TTSå¿«é€Ÿæ¨¡å¼ï¼ˆé»˜è®¤å¼€å¯ï¼‰

    Returns:
        (sample_rate, audio_array) æˆ– Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        mode_str = "fast" if fast_mode else "orpheus"
        LOG.info(f"[TTS] Calling Speech API ({mode_str}) for speaker: {speaker}, text: {text[:30]}...")
        
        response = requests.post(
            f"{SPEECH_API_URL}/synthesize/json",
            json={"text": text, "speaker": speaker, "fast_mode": fast_mode},
            timeout=120 if not fast_mode else 30  # fast modeéœ€è¦è¾ƒå°‘æ—¶é—´
        )
        
        if response.status_code != 200:
            LOG.error(f"[TTS] API error: {response.status_code} - {response.text}")
            return None
        
        result = response.json()
        audio_format = result.get("format", "wav")
        
        # è§£ç  base64 éŸ³é¢‘
        audio_bytes = base64.b64decode(result["audio_base64"])
        
        if audio_format == "mp3":
            # MP3æ ¼å¼ï¼ˆEdge-TTSè¿”å›ï¼‰- éœ€è¦ä½¿ç”¨pydubæˆ–å…¶ä»–æ–¹å¼è§£ç 
            try:
                from pydub import AudioSegment
                audio_segment = AudioSegment.from_mp3(io.BytesIO(audio_bytes))
                sample_rate = audio_segment.frame_rate
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                samples = np.array(audio_segment.get_array_of_samples(), dtype=np.float32)
                samples = samples / 32768.0  # å½’ä¸€åŒ–åˆ° [-1, 1]
                # å¦‚æœæ˜¯ç«‹ä½“å£°ï¼Œè½¬æ¢ä¸ºå•å£°é“
                if audio_segment.channels == 2:
                    samples = samples.reshape(-1, 2).mean(axis=1)
                LOG.info(f"[TTS] Successfully synthesized (MP3): {len(samples)} samples at {sample_rate}Hz")
                return (sample_rate, samples)
            except ImportError:
                LOG.warning("[TTS] pydub not installed, returning raw MP3 bytes")
                # å¦‚æœæ²¡æœ‰pydubï¼Œç›´æ¥è¿”å›MP3å­—èŠ‚ä¾›Gradioå¤„ç†
                return (24000, audio_bytes)
        else:
            # WAVæ ¼å¼ï¼ˆOrpheusè¿”å›ï¼‰- ä½¿ç”¨æ ‡å‡†åº“ wave
            import wave
            
            with wave.open(io.BytesIO(audio_bytes), 'rb') as wav_file:
                sample_rate = wav_file.getframerate()
                n_channels = wav_file.getnchannels()
                sample_width = wav_file.getsampwidth()
                n_frames = wav_file.getnframes()
                
                # è¯»å–åŸå§‹éŸ³é¢‘æ•°æ®
                raw_data = wav_file.readframes(n_frames)
                
                # æ ¹æ®æ ·æœ¬å®½åº¦è§£ææ•°æ®
                if sample_width == 2:  # 16-bit
                    audio_data = np.frombuffer(raw_data, dtype=np.int16).astype(np.float32) / 32768.0
                elif sample_width == 4:  # 32-bit
                    audio_data = np.frombuffer(raw_data, dtype=np.int32).astype(np.float32) / 2147483648.0
                else:  # 8-bit
                    audio_data = np.frombuffer(raw_data, dtype=np.uint8).astype(np.float32) / 128.0 - 1.0
                
                # å¦‚æœæ˜¯ç«‹ä½“å£°ï¼Œè½¬æ¢ä¸ºå•å£°é“
                if n_channels == 2:
                    audio_data = audio_data.reshape(-1, 2).mean(axis=1)
            
            LOG.info(f"[TTS] Successfully synthesized (WAV): {len(audio_data)} samples at {sample_rate}Hz")
            return (sample_rate, audio_data)
        
    except requests.exceptions.ConnectionError:
        LOG.error(f"[TTS] Cannot connect to Speech API at {SPEECH_API_URL}. Is it running?")
        return None
    except requests.exceptions.Timeout:
        LOG.error(f"[TTS] Request timed out. TTS model may be loading, please try again.")
        return None
    except Exception as e:
        LOG.error(f"[TTS] è¯­éŸ³åˆæˆå¤±è´¥: {e}")
        return None


def transcribe_audio(audio_data: Tuple[int, np.ndarray]) -> Optional[str]:
    """
    é€šè¿‡ Speech API è½¬å½•è¯­éŸ³

    Args:
        audio_data: (sample_rate, audio_array)

    Returns:
        è½¬å½•çš„æ–‡æœ¬æˆ– Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        sample_rate, audio = audio_data
        
        # ç¡®ä¿éŸ³é¢‘æ˜¯ float32 æ ¼å¼
        if audio.dtype != np.float32:
            audio = audio.astype(np.float32)
            # å¦‚æœæ˜¯æ•´æ•°æ ¼å¼ï¼Œå½’ä¸€åŒ–åˆ° [-1, 1]
            if np.abs(audio).max() > 1.0:
                audio = audio / 32768.0
        
        LOG.info(f"[STT] Calling Speech API for transcription")
        
        # å°†éŸ³é¢‘è½¬æ¢ä¸º WAV bytesï¼ˆä½¿ç”¨æ ‡å‡†åº“ waveï¼‰
        import wave
        buffer = io.BytesIO()
        
        # è½¬æ¢ä¸º 16-bit PCM
        audio_int16 = (audio * 32767).astype(np.int16)
        
        with wave.open(buffer, 'wb') as wav_file:
            wav_file.setnchannels(1)  # å•å£°é“
            wav_file.setsampwidth(2)  # 16-bit
            wav_file.setframerate(sample_rate)
            wav_file.writeframes(audio_int16.tobytes())
        
        buffer.seek(0)
        
        # å‘é€æ–‡ä»¶ - å‚æ•°åå¿…é¡»æ˜¯ "audio" ä¸ API å®šä¹‰åŒ¹é…
        files = {"audio": ("audio.wav", buffer, "audio/wav")}
        # å¢åŠ è¶…æ—¶æ—¶é—´ï¼š(è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶) - Whisper æ¨¡å‹é¦–æ¬¡åŠ è½½æˆ–å¤„ç†é•¿éŸ³é¢‘éœ€è¦æ›´å¤šæ—¶é—´
        response = requests.post(
            f"{SPEECH_API_URL}/transcribe",
            files=files,
            timeout=(10, 120)  # è¿æ¥10ç§’ï¼Œè¯»å–120ç§’
        )
        
        if response.status_code != 200:
            LOG.error(f"[STT] API error: {response.status_code} - {response.text}")
            return None
        
        result = response.json()
        text = result.get("text", "")
        
        LOG.info(f"[STT] Transcription result: {text[:50]}...")
        return text
        
    except requests.exceptions.ConnectionError:
        LOG.error(f"[STT] Cannot connect to Speech API at {SPEECH_API_URL}. Is it running?")
        return None
    except Exception as e:
        LOG.error(f"[STT] è¯­éŸ³è½¬å½•å¤±è´¥: {e}")
        return None


# åˆå§‹åŒ–åœºæ™¯ä»£ç†
agents = {
    "job_interview": ScenarioAgent("job_interview"),
    "hotel_checkin": ScenarioAgent("hotel_checkin"),
    "renting": ScenarioAgent("renting"),
    "salary_negotiation": ScenarioAgent("salary_negotiation"),
    # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šåœºæ™¯ä»£ç†
}

# éš¾åº¦çº§åˆ«é€‰é¡¹
DIFFICULTY_CHOICES = [
    ("åˆçº§ (A1/A2)", "primary"),
    ("ä¸­çº§ (B1/B2)", "medium"),
    ("é«˜çº§ (C1/C2)", "advanced"),
]

# å¯¹è¯è½®æ•°é€‰é¡¹
TURNS_CHOICES = [
    ("ç®€çŸ­ (10è½®)", 10),
    ("æ ‡å‡† (20è½®)", 20),
    ("æ‰©å±• (30è½®)", 30),
    ("æ·±åº¦ (50è½®)", 50),
]


def get_page_desc(scenario):
    try:
        with open(f"content/page/{scenario}.md", "r", encoding="utf-8") as file:
            scenario_intro = file.read().strip()
        return scenario_intro
    except FileNotFoundError:
        LOG.error(f"åœºæ™¯ä»‹ç»æ–‡ä»¶ content/page/{scenario}.md æœªæ‰¾åˆ°ï¼")
        return "åœºæ™¯ä»‹ç»æ–‡ä»¶æœªæ‰¾åˆ°ã€‚"


def build_config_from_ui(turns: int, difficulty: str) -> ConversationConfig:
    """ä» UI æ§ä»¶å€¼æ„å»ºä¼šè¯é…ç½®ã€‚"""
    return create_config(turns=turns, difficulty=difficulty)


# è·å–åœºæ™¯ä»‹ç»å¹¶å¯åŠ¨æ–°ä¼šè¯çš„å‡½æ•°
def start_new_scenario_chatbot(scenario, turns, difficulty):
    """
    åˆ‡æ¢åœºæ™¯æ—¶å¯åŠ¨æ–°çš„èŠå¤©ä¼šè¯ï¼Œæ¸…é™¤ä¹‹å‰çš„èŠå¤©å†å²ã€‚

    å‚æ•°:
        scenario: åœºæ™¯åç§°
        turns: å¯¹è¯è½®æ•°
        difficulty: éš¾åº¦çº§åˆ«

    è¿”å›:
        list: åŒ…å«åˆå§‹AIæ¶ˆæ¯çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œç”¨äºé‡ç½®èŠå¤©ç•Œé¢
    """
    LOG.info(f"[Scenario] Switching to scenario: {scenario}, turns={turns}, difficulty={difficulty}")

    # åˆ›å»ºé…ç½®
    config = build_config_from_ui(turns, difficulty)

    # å¯åŠ¨æ–°åœºæ™¯çš„ä¼šè¯å¹¶æ¸…é™¤å†å²
    # è¿™ä¼šæ¸…é™¤å½“å‰åœºæ™¯çš„ä¼šè¯å†å²ï¼Œç¡®ä¿æ¯æ¬¡åˆ‡æ¢åœºæ™¯éƒ½æ˜¯å…¨æ–°çš„ä¼šè¯
    initial_ai_message = agents[scenario].start_new_session(config=config)

    # Gradio 6.0.0 ä½¿ç”¨å­—å…¸æ ¼å¼çš„æ¶ˆæ¯
    # è¿”å›æ–°çš„æ¶ˆæ¯åˆ—è¡¨ä¼šæ›¿æ¢èŠå¤©æœºå™¨äººä¸­çš„æ‰€æœ‰å†å²æ¶ˆæ¯
    # è¿™åº”è¯¥æ¸…é™¤ ChatInterface ä¸­æ˜¾ç¤ºçš„æ—§æ¶ˆæ¯
    LOG.debug(f"[Scenario] Returning new initial message for chatbot reset")
    return [{"role": "assistant", "content": initial_ai_message}]


def create_scenario_tab():
    with gr.Tab("åœºæ™¯"):  # åœºæ™¯æ ‡ç­¾
        gr.Markdown("## é€‰æ‹©ä¸€ä¸ªåœºæ™¯å®Œæˆç›®æ ‡å’ŒæŒ‘æˆ˜")  # åœºæ™¯é€‰æ‹©è¯´æ˜

        with gr.Row():
            # å·¦ä¾§è¾¹æ ï¼šé…ç½®é€‰é¡¹
            with gr.Column(scale=1):
                gr.Markdown("### ä¼šè¯è®¾ç½®")

                # åœºæ™¯é€‰æ‹©
                scenario_radio = gr.Radio(
                    choices=[
                        ("æ±‚èŒé¢è¯•", "job_interview"),
                        ("é…’åº—å…¥ä½", "hotel_checkin"),
                        ("ç§Ÿæˆ¿", "renting"),
                        ("è–ªèµ„è°ˆåˆ¤", "salary_negotiation"),
                    ],
                    label="åœºæ™¯",
                    value="job_interview",
                )

                # éš¾åº¦çº§åˆ«ä¸‹æ‹‰èœå•
                difficulty_dropdown = gr.Dropdown(
                    choices=DIFFICULTY_CHOICES,
                    label="éš¾åº¦çº§åˆ«",
                    value="medium",
                    info="é€‰æ‹©è¯­è¨€éš¾åº¦ï¼šåˆçº§(A1/A2)ã€ä¸­çº§(B1/B2)ã€é«˜çº§(C1/C2)",
                )

                # å¯¹è¯è½®æ•°æ»‘å—
                turns_slider = gr.Slider(
                    minimum=10,
                    maximum=50,
                    step=10,
                    value=20,
                    label="å¯¹è¯è½®æ•°",
                    info="é€‰æ‹©å¯¹è¯è½®æ•°ï¼Œå®Œæˆåä¼šæ”¶åˆ°åé¦ˆ",
                )

                # Speaker é€‰æ‹©ï¼ˆTTS è¯­éŸ³ï¼‰
                gr.Markdown("### è¯­éŸ³è®¾ç½®")
                speaker_dropdown = gr.Dropdown(
                    choices=SPEAKER_CHOICES,
                    label="TTS è¯­éŸ³è§’è‰²",
                    value="Ceylia",
                    info="é€‰æ‹© AI å›å¤çš„è¯­éŸ³è§’è‰²",
                )

                # TTS å¼€å…³
                tts_enabled = gr.Checkbox(
                    label="å¯ç”¨è¯­éŸ³æ’­æ”¾",
                    value=True,  # é»˜è®¤å¼€å¯
                    info="å¼€å¯å AI å›å¤ä¼šè‡ªåŠ¨ç”Ÿæˆå¹¶æ’­æ”¾è¯­éŸ³",
                )

                # å¼€å§‹æ–°ä¼šè¯æŒ‰é’®
                start_btn = gr.Button("å¼€å§‹æ–°ä¼šè¯", variant="primary")

                # åœºæ™¯ä»‹ç»
                scenario_intro = gr.Markdown()

            # å³ä¾§ï¼šèŠå¤©ç•Œé¢
            with gr.Column(scale=2):
                # ä½¿ç”¨ State æ¥è·Ÿè¸ªå½“å‰åœºæ™¯
                current_scenario_state = gr.State(value=None)
                # å­˜å‚¨æœ€åä¸€æ¡ AI æ¶ˆæ¯ç”¨äº TTS
                last_ai_message_state = gr.State(value="")

                scenario_chatbot = gr.Chatbot(
                    placeholder="<strong>ä½ çš„è‹±è¯­ç§æ•™ LangCoach</strong><br><br>é€‰æ‹©åœºæ™¯åå¼€å§‹å¯¹è¯å§ï¼",
                    height=450,
                    value=None,
                )

                # TTS æ’­æ”¾åŒºåŸŸï¼šæ’­æ”¾æŒ‰é’® + éŸ³é¢‘æ’­æ”¾å™¨
                with gr.Row():
                    tts_play_btn = gr.Button("ğŸ”Š æ’­æ”¾ AI è¯­éŸ³", variant="secondary", scale=1)
                    audio_output = gr.Audio(
                        label="AI è¯­éŸ³",
                        type="numpy",
                        autoplay=True,
                        scale=3,
                        elem_id="ai_audio_player",
                    )
                
                # JavaScript å¼ºåˆ¶è‡ªåŠ¨æ’­æ”¾ï¼ˆç»•è¿‡æµè§ˆå™¨é™åˆ¶ï¼‰
                gr.HTML("""
                <script>
                // ç›‘å¬éŸ³é¢‘å…ƒç´ å˜åŒ–ï¼Œå°è¯•è‡ªåŠ¨æ’­æ”¾
                const observer = new MutationObserver((mutations) => {
                    const audioContainer = document.getElementById('ai_audio_player');
                    if (audioContainer) {
                        const audio = audioContainer.querySelector('audio');
                        if (audio && audio.src && audio.paused) {
                            audio.play().catch(e => console.log('Autoplay blocked:', e));
                        }
                    }
                });
                
                // å¼€å§‹è§‚å¯Ÿ
                setTimeout(() => {
                    const target = document.getElementById('ai_audio_player');
                    if (target) {
                        observer.observe(target, { childList: true, subtree: true, attributes: true });
                    }
                }, 1000);
                </script>
                """)

                # æ‰‹åŠ¨åˆ›å»ºèŠå¤©è¾“å…¥æ¡†å’Œå‘é€æŒ‰é’®ï¼Œæ”¾åœ¨åŒä¸€è¡Œ
                with gr.Row():
                    scenario_input = gr.Textbox(
                        placeholder="è¾“å…¥ä½ çš„æ¶ˆæ¯...",
                        label="æ¶ˆæ¯",
                        scale=7,
                        container=False,
                    )
                    scenario_submit_btn = gr.Button("å‘é€", variant="primary", scale=1, min_width=80)

                # è¯­éŸ³è¾“å…¥åŒºåŸŸ
                with gr.Row():
                    audio_input = gr.Audio(
                        label="ğŸ¤ è¯­éŸ³è¾“å…¥ï¼ˆå½•éŸ³åç‚¹å‡»å‘é€è¯­éŸ³ï¼‰",
                        sources=["microphone"],
                        type="numpy",
                        scale=3,
                    )
                    voice_submit_btn = gr.Button("å‘é€è¯­éŸ³", variant="secondary", scale=1, min_width=80)

        # æ›´æ–°åœºæ™¯ä»‹ç»å¹¶åœ¨åœºæ™¯å˜åŒ–æ—¶å¯åŠ¨æ–°ä¼šè¯
        def on_scenario_change(scenario, current_state, turns, difficulty):
            """å¤„ç†åœºæ™¯åˆ‡æ¢ï¼Œé‡ç½®èŠå¤©ç•Œé¢"""
            LOG.info(f"[Scenario] Scenario changed from {current_state} to: {scenario}")

            # å¦‚æœåœºæ™¯å‘ç”Ÿå˜åŒ–ï¼Œæ¸…é™¤ä¹‹å‰åœºæ™¯çš„ä¼šè¯å†å²
            if current_state and current_state != scenario and current_state in agents:
                LOG.debug(f"[Scenario] Clearing previous scenario history: {current_state}")
                agents[current_state].start_new_session()  # æ¸…é™¤ä¹‹å‰åœºæ™¯çš„å†å²

            # å¯åŠ¨æ–°åœºæ™¯çš„ä¼šè¯
            intro = get_page_desc(scenario)
            new_chat_history = start_new_scenario_chatbot(scenario, turns, difficulty)

            # è·å–åˆå§‹ AI æ¶ˆæ¯
            initial_ai_message = ""
            if new_chat_history and len(new_chat_history) > 0:
                initial_ai_message = new_chat_history[0].get("content", "")

            LOG.debug(f"[Scenario] Returning intro and new chat history for scenario: {scenario}, history: {new_chat_history}")
            # ç›´æ¥è¿”å›æ–°æ¶ˆæ¯åˆ—è¡¨ï¼Œè¿™ä¼šæ›¿æ¢ chatbot ä¸­çš„æ‰€æœ‰æ—§æ¶ˆæ¯
            # åŒæ—¶æ¸…ç©ºéŸ³é¢‘è¾“å‡ºï¼Œä¿å­˜åˆå§‹ AI æ¶ˆæ¯
            return intro, new_chat_history, scenario, None, initial_ai_message

        scenario_radio.change(
            fn=on_scenario_change,
            inputs=[scenario_radio, current_scenario_state, turns_slider, difficulty_dropdown],
            outputs=[scenario_intro, scenario_chatbot, current_scenario_state, audio_output, last_ai_message_state],
        )

        # å¼€å§‹æ–°ä¼šè¯æŒ‰é’®ç‚¹å‡»äº‹ä»¶
        def on_start_new_session(scenario, turns, difficulty):
            """æ‰‹åŠ¨å¼€å§‹æ–°ä¼šè¯"""
            LOG.info(f"[Scenario] Starting new session: {scenario}, turns={turns}, difficulty={difficulty}")
            intro = get_page_desc(scenario)
            new_chat_history = start_new_scenario_chatbot(scenario, turns, difficulty)

            # è·å–åˆå§‹ AI æ¶ˆæ¯
            initial_ai_message = ""
            if new_chat_history and len(new_chat_history) > 0:
                initial_ai_message = new_chat_history[0].get("content", "")

            return intro, new_chat_history, None, initial_ai_message

        start_btn.click(
            fn=on_start_new_session,
            inputs=[scenario_radio, turns_slider, difficulty_dropdown],
            outputs=[scenario_intro, scenario_chatbot, audio_output, last_ai_message_state],
        )

        # TTS æ’­æ”¾æŒ‰é’®ç‚¹å‡»äº‹ä»¶
        def on_tts_play(last_message, speaker):
            """ç‚¹å‡»æ’­æ”¾æŒ‰é’®æ—¶ç”Ÿæˆ TTS"""
            if not last_message:
                LOG.warning("[Scenario] No AI message to play")
                return None

            # æå–çº¯è‹±æ–‡å›å¤éƒ¨åˆ†ï¼ˆæ’é™¤å¯¹è¯æç¤ºï¼‰
            english_response = extract_english_response(last_message)
            LOG.info(f"[Scenario] Playing TTS for: {english_response[:50]}...")
            audio_result = synthesize_speech(english_response, speaker)
            return audio_result

        tts_play_btn.click(
            fn=on_tts_play,
            inputs=[last_ai_message_state, speaker_dropdown],
            outputs=[audio_output],
        )

        # å¤„ç†ç”¨æˆ·æ¶ˆæ¯çš„å‡½æ•°ï¼ˆæ”¯æŒ TTSï¼‰
        def on_message_submit(user_input, chat_history, scenario, speaker, enable_tts):
            """å¤„ç†ç”¨æˆ·æäº¤çš„æ¶ˆæ¯"""
            if not user_input or not user_input.strip():
                return chat_history or [], "", None, ""

            LOG.debug(f"[Scenario] User message submitted for scenario: {scenario}")

            # ç¡®ä¿ chat_history ä¸ä¸º None
            if chat_history is None:
                chat_history = []

            # åˆ›å»ºæ–°çš„èŠå¤©å†å²åˆ—è¡¨ï¼ˆé¿å…ç›´æ¥ä¿®æ”¹åŸåˆ—è¡¨ï¼‰
            new_chat_history = list(chat_history) if chat_history else []

            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°èŠå¤©å†å²
            new_chat_history.append({"role": "user", "content": user_input.strip()})

            # è·å–AIå›å¤
            bot_message = agents[scenario].chat_with_history(user_input.strip())

            # æ·»åŠ AIå›å¤åˆ°èŠå¤©å†å²
            new_chat_history.append({"role": "assistant", "content": bot_message})

            LOG.debug(f"[Scenario] Chat history updated, length: {len(new_chat_history)}")

            # å¦‚æœå¯ç”¨ TTSï¼Œè‡ªåŠ¨ç”Ÿæˆè¯­éŸ³ï¼ˆåªå¯¹è‹±æ–‡å›å¤éƒ¨åˆ†ï¼‰
            audio_result = None
            if enable_tts:
                english_response = extract_english_response(bot_message)
                LOG.info(f"[Scenario] Auto-generating TTS for: {english_response[:50]}...")
                audio_result = synthesize_speech(english_response, speaker)

            return new_chat_history, "", audio_result, bot_message

        # å¤„ç†è¯­éŸ³è¾“å…¥çš„å‡½æ•°
        def on_voice_submit(audio_data, chat_history, scenario, speaker, enable_tts):
            """å¤„ç†è¯­éŸ³è¾“å…¥"""
            if audio_data is None:
                return chat_history or [], None, None, ""

            LOG.info("[Scenario] Processing voice input...")

            # è½¬å½•è¯­éŸ³
            transcribed_text = transcribe_audio(audio_data)
            if not transcribed_text:
                LOG.warning("[Scenario] Voice transcription failed or empty")
                return chat_history or [], None, None, ""

            LOG.info(f"[Scenario] Transcribed text: {transcribed_text}")

            # ä½¿ç”¨è½¬å½•çš„æ–‡æœ¬è¿›è¡Œå¯¹è¯
            new_chat_history, _, audio_result, bot_message = on_message_submit(
                transcribed_text, chat_history, scenario, speaker, enable_tts
            )

            # æ¸…ç©ºéŸ³é¢‘è¾“å…¥
            return new_chat_history, None, audio_result, bot_message

        # ç»‘å®šæ–‡æœ¬æäº¤äº‹ä»¶
        scenario_submit_btn.click(
            fn=on_message_submit,
            inputs=[scenario_input, scenario_chatbot, scenario_radio, speaker_dropdown, tts_enabled],
            outputs=[scenario_chatbot, scenario_input, audio_output, last_ai_message_state],
        )

        # ä¹Ÿæ”¯æŒå›è½¦é”®æäº¤
        scenario_input.submit(
            fn=on_message_submit,
            inputs=[scenario_input, scenario_chatbot, scenario_radio, speaker_dropdown, tts_enabled],
            outputs=[scenario_chatbot, scenario_input, audio_output, last_ai_message_state],
        )

        # ç»‘å®šè¯­éŸ³æäº¤äº‹ä»¶
        voice_submit_btn.click(
            fn=on_voice_submit,
            inputs=[audio_input, scenario_chatbot, scenario_radio, speaker_dropdown, tts_enabled],
            outputs=[scenario_chatbot, audio_input, audio_output, last_ai_message_state],
        )
