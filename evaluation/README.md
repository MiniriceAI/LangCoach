# LangCoach Evaluation Framework

## æ¦‚è¿°

LangCoach Evaluation Framework æ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°ç³»ç»Ÿï¼Œç”¨äºè¡¡é‡ LangCoach Agent çš„æ€§èƒ½å’Œè´¨é‡ã€‚

### æ ¸å¿ƒåŠŸèƒ½

1. **E2E Audio Latency æµ‹é‡** - è¡¡é‡å®Œæ•´ Audio Pipeline çš„ç«¯åˆ°ç«¯å»¶è¿Ÿ
2. **æ¨¡å—åŒ–è¯„ä¼°** - ç‹¬ç«‹è¯„ä¼° STTã€LLMã€TTS å„ä¸ªæ¨¡å—
3. **å¯¹æ¯”æµ‹è¯•** - æ”¯æŒä¸åŒ LLM Provider å’Œ TTS æ¨¡å¼çš„ A/B æµ‹è¯•
4. **å›ºå®šåŸºå‡†æ•°æ®é›†** - 100 æ¡å›ºå®šæµ‹è¯•æ•°æ®ï¼Œæ”¯æŒé…ç½®è¿è¡Œå‰ n æ¡
5. **å¤šæ ¼å¼æŠ¥å‘Š** - æ”¯æŒ JSONã€Markdownã€HTML æ ¼å¼çš„æµ‹è¯•æŠ¥å‘Š

### ç›®æ ‡æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | è¯´æ˜ |
|------|--------|------|
| E2E Audio Latency | < 3000ms | ä»ç”¨æˆ·åœæ­¢è¯´è¯åˆ° AI å¼€å§‹æ’­æ”¾éŸ³é¢‘ |
| LLM TTFT | < 1500ms | Time to First Token |
| Success Rate | > 99.5% | è¯·æ±‚æˆåŠŸç‡ |

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. é…ç½®ç¯å¢ƒå˜é‡

ç¡®ä¿ `.env` æ–‡ä»¶ä¸­é…ç½®äº†å¿…è¦çš„ç¯å¢ƒå˜é‡ï¼š

```bash
# LLM Provider (è‡³å°‘é…ç½®ä¸€ä¸ª)
DEEPSEEK_API_KEY=your_key_here
# æˆ–
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=your_model

# Speech API
SPEECH_API_URL=http://localhost:8600
```

### 3. å¯åŠ¨ Speech APIï¼ˆå¦‚éœ€æµ‹è¯• STT/TTSï¼‰

```bash
python -m src.api.speech_api
```

### 4. è¿è¡Œè¯„ä¼°

```bash
# å¿«é€Ÿæµ‹è¯•ï¼ˆ5 æ¡æ ·æœ¬ï¼‰
python -m evaluation.run_eval --quick

# å®Œæ•´è¯„ä¼°ï¼ˆ100 æ¡æ ·æœ¬ï¼‰
python -m evaluation.run_eval

# ä»…è¯„ä¼° LLM
python -m evaluation.run_eval --module llm

# è¯„ä¼° E2E Pipeline
python -m evaluation.run_eval --module e2e

# å¯¹æ¯”æµ‹è¯•
python -m evaluation.run_eval --compare --providers deepseek ollama
```

## ç›®å½•ç»“æ„

```
evaluation/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ run_eval.py              # CLI å…¥å£
â”œâ”€â”€ benchmark/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dataset.py           # åŸºå‡†æ•°æ®é›†ç®¡ç†
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ benchmark_samples.json  # 100 æ¡å›ºå®šæµ‹è¯•æ•°æ®
â”œâ”€â”€ evaluators/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py              # åŸºç¡€è¯„ä¼°å™¨ç±»
â”‚   â”œâ”€â”€ llm_evaluator.py     # LLM è¯„ä¼°å™¨
â”‚   â”œâ”€â”€ tts_evaluator.py     # TTS è¯„ä¼°å™¨
â”‚   â”œâ”€â”€ stt_evaluator.py     # STT è¯„ä¼°å™¨
â”‚   â””â”€â”€ e2e_evaluator.py     # E2E Pipeline è¯„ä¼°å™¨
â”œâ”€â”€ runners/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ evaluation_runner.py # è¯„ä¼°è¿è¡Œå™¨
â”‚   â””â”€â”€ comparison_runner.py # å¯¹æ¯”æµ‹è¯•è¿è¡Œå™¨
â””â”€â”€ reports/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ report_generator.py  # æŠ¥å‘Šç”Ÿæˆå™¨
    â””â”€â”€ results/             # è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•
```

## ä½¿ç”¨æŒ‡å—

### å‘½ä»¤è¡Œå‚æ•°

```bash
python -m evaluation.run_eval [OPTIONS]

é€‰é¡¹:
  --module, -m {llm,tts,stt,e2e,all}  è¯„ä¼°æ¨¡å— (é»˜è®¤: all)
  --samples, -n INT                   æ ·æœ¬æ•°é‡ (é»˜è®¤: 100)
  --quick, -q                         å¿«é€Ÿæ¨¡å¼ (5 æ¡æ ·æœ¬)
  --provider, -p STR                  LLM Provider (ollama/deepseek/openai)
  --compare, -c                       è¿è¡Œå¯¹æ¯”æµ‹è¯•
  --providers STR [STR ...]           å¯¹æ¯”çš„ Provider åˆ—è¡¨
  --tts-mode {fast,local}             TTS æ¨¡å¼ (é»˜è®¤: fast)
  --output, -o STR                    è¾“å‡ºç›®å½•
  --report, -r [FORMAT ...]           ç”ŸæˆæŠ¥å‘Šæ ¼å¼ (json/md/html/txt)
  --verbose, -v                       è¯¦ç»†è¾“å‡º
  --silent, -s                        é™é»˜æ¨¡å¼
```

### ç¤ºä¾‹

#### 1. è¿è¡Œå®Œæ•´è¯„ä¼°

```bash
python -m evaluation.run_eval
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
============================================================
 LangCoach Evaluation Framework
 2024-01-15 10:30:00
============================================================

============================================================
LLM Evaluation
Provider: auto
Samples: 100
============================================================

Progress: 100/100 (100.0%)

Results:
  Success Rate: 100.0%
  Mean Latency: 850ms
  P95 Latency: 1200ms
```

#### 2. ä»…è¯„ä¼° LLM æ¨¡å—

```bash
python -m evaluation.run_eval --module llm --samples 20
```

#### 3. å¯¹æ¯” DeepSeek å’Œ Ollama

```bash
python -m evaluation.run_eval --compare --providers deepseek ollama -n 50
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
============================================================
LLM Provider Comparison
Providers: deepseek, ollama
Samples: 50
============================================================

[deepseek] Progress: 50/50 (100.0%)
[ollama] Progress: 50/50 (100.0%)

============================================================
Comparison Results:
============================================================

deepseek:
  Success Rate: 100.0%
  Mean Latency: 650ms
  P95 Latency: 950ms

ollama:
  Success Rate: 100.0%
  Mean Latency: 1200ms
  P95 Latency: 1800ms

ğŸ† Fastest: deepseek (650ms mean)
```

#### 4. E2E Pipeline è¯„ä¼°

```bash
python -m evaluation.run_eval --module e2e --report html
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
============================================================
E2E Pipeline Evaluation
LLM Provider: auto
TTS Mode: Edge-TTS (fast)
Samples: 100
Target Latency: < 3000ms
============================================================

Progress: 100/100 (100.0%)

Results:
  Success Rate: 98.0%
  Within Target (<3s): 95.0%

  Timing Breakdown:
    STT Mean: 800ms
    LLM Mean: 900ms
    TTS Mean: 400ms
    Total Mean: 2100ms
    Total P95: 2800ms
```

#### 5. ç”Ÿæˆå¤šæ ¼å¼æŠ¥å‘Š

```bash
python -m evaluation.run_eval --report json md html
```

### ç¼–ç¨‹æ¥å£

```python
from evaluation.runners import EvaluationRunner, ComparisonRunner
from evaluation.reports import ReportGenerator

# åˆ›å»ºè¯„ä¼°è¿è¡Œå™¨
runner = EvaluationRunner()

# è¿è¡Œ LLM è¯„ä¼°
result = runner.run_llm_evaluation(n_samples=50, provider="deepseek")
print(f"Mean Latency: {result.timing.mean * 1000:.0f}ms")

# è¿è¡Œ E2E è¯„ä¼°
e2e_result = runner.run_e2e_evaluation(n_samples=20)
print(f"Within Target: {e2e_result.extra_metrics['within_target_rate']:.1f}%")

# å¯¹æ¯”æµ‹è¯•
comparison = ComparisonRunner()
results = comparison.compare_llm_providers(["deepseek", "ollama"], n_samples=30)

# ç”ŸæˆæŠ¥å‘Š
generator = ReportGenerator()
saved = generator.save_report(results, "comparison", formats=["html", "md"])
```

## åŸºå‡†æ•°æ®é›†

### æ•°æ®é›†ç»“æ„

åŸºå‡†æ•°æ®é›†åŒ…å« 100 æ¡å›ºå®šçš„æµ‹è¯•æ ·æœ¬ï¼Œåˆ†ä¸º 4 ä¸ªåœºæ™¯ï¼š

| åœºæ™¯ | æ ·æœ¬æ•° | éš¾åº¦ |
|------|--------|------|
| Job Interview | 25 | Medium |
| Hotel Check-in | 25 | Primary |
| Renting | 25 | Medium |
| Salary Negotiation | 25 | Advanced |

### è‡ªå®šä¹‰æ•°æ®é›†

æ•°æ®é›†å­˜å‚¨åœ¨ `evaluation/benchmark/data/benchmark_samples.json`ï¼Œé¦–æ¬¡è¿è¡Œæ—¶è‡ªåŠ¨ç”Ÿæˆã€‚

å¯ä»¥é€šè¿‡ä¿®æ”¹ `dataset.py` ä¸­çš„ `_create_default_dataset()` æ–¹æ³•æ¥è‡ªå®šä¹‰æ•°æ®é›†ã€‚

### è¿è¡Œéƒ¨åˆ†æ ·æœ¬

```bash
# è¿è¡Œå‰ 10 æ¡
python -m evaluation.run_eval -n 10

# è¿è¡Œå‰ 5 æ¡ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
python -m evaluation.run_eval --quick
```

## è¯„ä¼°æŒ‡æ ‡

### æ—¶é—´æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ |
|------|------|
| Mean | å¹³å‡å»¶è¿Ÿ |
| Median | ä¸­ä½æ•°å»¶è¿Ÿ |
| P50/P90/P95/P99 | ç™¾åˆ†ä½å»¶è¿Ÿ |
| Min/Max | æœ€å°/æœ€å¤§å»¶è¿Ÿ |
| Std | æ ‡å‡†å·® |

### E2E åˆ†è§£æŒ‡æ ‡

| ç»„ä»¶ | è¯´æ˜ |
|------|------|
| STT Latency | è¯­éŸ³è½¬æ–‡å­—å»¶è¿Ÿ |
| LLM Latency | LLM æ¨ç†å»¶è¿Ÿ |
| TTS Latency | æ–‡å­—è½¬è¯­éŸ³å»¶è¿Ÿ |
| Total Latency | æ€»ç«¯åˆ°ç«¯å»¶è¿Ÿ |

### è´¨é‡æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ |
|------|------|
| Success Rate | è¯·æ±‚æˆåŠŸç‡ |
| Within Target Rate | è¾¾åˆ°ç›®æ ‡å»¶è¿Ÿçš„æ¯”ä¾‹ |
| WER (STT) | Word Error Rate |
| RTF (TTS) | Real-Time Factor |

## æŠ¥å‘Šæ ¼å¼

### JSON æŠ¥å‘Š

```json
{
  "llm": {
    "evaluator": "LLMEvaluator",
    "provider": "deepseek",
    "model": "deepseek-chat",
    "timing": {
      "count": 100,
      "mean_ms": 850.5,
      "median_ms": 800.0,
      "p95_ms": 1200.0
    },
    "success_rate": 100.0
  }
}
```

### Markdown æŠ¥å‘Š

ç”Ÿæˆæ ¼å¼åŒ–çš„ Markdown è¡¨æ ¼ï¼Œé€‚åˆåœ¨ GitHub æˆ–æ–‡æ¡£ä¸­å±•ç¤ºã€‚

### HTML æŠ¥å‘Š

ç”Ÿæˆå¸¦æ ·å¼çš„ HTML é¡µé¢ï¼ŒåŒ…å«äº¤äº’å¼å›¾è¡¨å’Œè¯¦ç»†æŒ‡æ ‡ã€‚

## é…ç½®é€‰é¡¹

### ç¯å¢ƒå˜é‡

```bash
# è¯„ä¼°è¾“å‡ºç›®å½•
EVAL_OUTPUT_DIR=evaluation/reports

# E2E å»¶è¿Ÿç›®æ ‡ï¼ˆæ¯«ç§’ï¼‰
EVAL_TARGET_LATENCY_MS=3000

# TTS è¯„ä¼°æ¨¡å¼
EVAL_TTS_MODE=fast

# æŠ¥å‘Šæ ¼å¼
EVAL_REPORT_FORMATS=json,md,html
```

## æœ€ä½³å®è·µ

### 1. åŸºå‡†æµ‹è¯•

- é¦–æ¬¡è¿è¡Œå®Œæ•´çš„ 100 æ¡æ ·æœ¬å»ºç«‹åŸºå‡†
- ä¿å­˜ç»“æœä½œä¸ºåç»­å¯¹æ¯”çš„å‚è€ƒ

### 2. è¿­ä»£ä¼˜åŒ–

- ä¿®æ”¹é…ç½®åè¿è¡Œå¿«é€Ÿæµ‹è¯• (`--quick`)
- ç¡®è®¤æ”¹è¿›åè¿è¡Œå®Œæ•´æµ‹è¯•

### 3. å¯¹æ¯”æµ‹è¯•

- ä½¿ç”¨ç›¸åŒçš„æ ·æœ¬æ•°è¿›è¡Œå¯¹æ¯”
- å¤šæ¬¡è¿è¡Œå–å¹³å‡å€¼å‡å°‘æ³¢åŠ¨

### 4. æŒç»­é›†æˆ

- åœ¨ CI/CD ä¸­é›†æˆè¯„ä¼°
- è®¾ç½®æ€§èƒ½å›å½’å‘Šè­¦

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **Speech API è¿æ¥å¤±è´¥**
   ```
   ç¡®ä¿ Speech API æ­£åœ¨è¿è¡Œï¼š
   python -m src.api.speech_api
   ```

2. **LLM Provider ä¸å¯ç”¨**
   ```
   æ£€æŸ¥ .env é…ç½®å’Œ API Key
   è¿è¡Œ: python -c "from agents.llm_factory import list_available_providers; print(list_available_providers())"
   ```

3. **å†…å­˜ä¸è¶³**
   ```
   å‡å°‘æ ·æœ¬æ•°: --samples 10
   æˆ–ä½¿ç”¨ API æ¨¡å¼è€Œéæœ¬åœ°æ¨¡å‹
   ```

## æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„è¯„ä¼°å™¨

1. ç»§æ‰¿ `BaseEvaluator` ç±»
2. å®ç° `initialize()`, `evaluate_single()`, `get_provider_info()` æ–¹æ³•
3. åœ¨ `evaluators/__init__.py` ä¸­å¯¼å‡º

### æ·»åŠ æ–°çš„æŠ¥å‘Šæ ¼å¼

1. åœ¨ `ReportGenerator` ä¸­æ·»åŠ  `generate_xxx_report()` æ–¹æ³•
2. åœ¨ `save_report()` ä¸­æ·»åŠ æ ¼å¼å¤„ç†

## ç‰ˆæœ¬å†å²

- **v1.0.0** - åˆå§‹ç‰ˆæœ¬
  - æ”¯æŒ STTã€LLMã€TTSã€E2E è¯„ä¼°
  - æ”¯æŒ DeepSeekã€Ollamaã€OpenAI å¯¹æ¯”
  - æ”¯æŒ JSONã€Markdownã€HTML æŠ¥å‘Š
