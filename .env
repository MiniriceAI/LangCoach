# LangCoach Environment Configuration
# Copy this file to .env and fill in your actual values

# ============================================
# Phase 2: Long-Term Memory Configuration
# ============================================

# Milvus Vector Database (Phase 2 新增)
MILVUS_HOST=milvus                 # Docker 内使用 "milvus"，本地开发使用 "localhost"
MILVUS_PORT=19530                  # Milvus 默认端口

# ============================================
# LLM Provider Configuration
# ============================================
# 重要：所有 LLM 配置都在此 .env 文件中设置
# 代码中不包含硬编码的默认值

# 提供者优先级
# 格式：逗号分隔的提供者列表，从左到右优先级递减
# 默认：ollama,deepseek,openai
LLM_PROVIDER_PRIORITY=deepseek,ollama,openai

# ---------------------------------------------
# 选项 1: Ollama (本地部署 - 默认首选)
# ---------------------------------------------
# 优势：免费、隐私、无需 API key
# 必须配置 OLLAMA_MODEL 和 OLLAMA_BASE_URL 才能启用

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=hf.co/unsloth/GLM-4-9B-0414-GGUF:Q8_K_XL
OLLAMA_TEMPERATURE=0.8
OLLAMA_MAX_TOKENS=8192
OLLAMA_ENABLED=true              # 默认启用，设为 false 可禁用

# 安装 Ollama:
# 1. 访问 https://ollama.com 下载安装
# 2. 运行: ollama serve
# 3. 拉取模型: ollama pull unsloth/GLM-4-9B-0414-GGUF:Q8_K_XL
# 4. 验证: ollama list

# 其他推荐模型：
# - llama3.1:8b-instruct-q8_0  (Llama 3.1 8B)
# - qwen2.5:14b                (Qwen 2.5 14B)
# - mistral:7b-instruct-v0.3   (Mistral 7B)

# ---------------------------------------------
# 选项 2: DeepSeek API (高性价比)
# ---------------------------------------------
# 优势：便宜、性能好、支持中文
# 官网：https://platform.deepseek.com
# 必须配置有效的 DEEPSEEK_API_KEY 才能启用

DEEPSEEK_API_KEY=your_deepseek_api_key_here
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_BASE_URL=https://api.deepseek.com
DEEPSEEK_TEMPERATURE=0.8
DEEPSEEK_MAX_TOKENS=8192
# DEEPSEEK_ENABLED=true            # 默认启用，设为 false 可禁用

# ---------------------------------------------
# 选项 3: OpenAI API
# ---------------------------------------------
# 优势：性能强、稳定性好
# 官网：https://platform.openai.com
# 必须配置有效的 OPENAI_API_KEY 才能启用

OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini
# OPENAI_BASE_URL=                 # 可选：用于兼容的 API（如 Azure OpenAI）
OPENAI_TEMPERATURE=0.8
OPENAI_MAX_TOKENS=8192
# OPENAI_ENABLED=true              # 默认启用，设为 false 可禁用

# 推荐模型：
# - gpt-4o-mini    (便宜、快速)
# - gpt-4o         (性能最佳)
# - gpt-4-turbo    (长上下文)
# - gpt-3.5-turbo  (最便宜)

# ============================================
# Application Configuration
# ============================================

# Gradio Web Server
GRADIO_PORT=8400                   # Phase 2 默认端口改为 8300
# GRADIO_FORCE_RESTART=true         # 设置为 true 自动停止占用端口的进程

# Python Configuration
PYTHONUNBUFFERED=1                 # 禁用 Python 输出缓冲

# ============================================
# Docker Volume Directory (可选)
# ============================================

# 用于存储 Milvus/etcd/minio 数据
# DOCKER_VOLUME_DIRECTORY=./volumes

# ============================================
# Memory System Configuration (高级)
# ============================================

# 使用哪个嵌入模型（如果同时配置了 OpenAI 和 Ollama）
# 默认：优先使用 OpenAI (更准确)
# USE_OLLAMA_EMBEDDINGS=false

# 上下文窗口限制（单位：tokens）
# MAX_CONTEXT_TOKENS=3000

# ============================================
# Development Configuration
# ============================================

# 日志级别 (DEBUG, INFO, WARNING, ERROR)
# LOG_LEVEL=INFO

# 是否启用详细调试日志
# DEBUG=false

# ============================================
# 快速开始指南
# ============================================

# 推荐方案 1：使用 Ollama（完全免费，本地运行）
# 1. 安装 Ollama: https://ollama.com
# 2. 启动服务: ollama serve
# 3. 下载模型: ollama pull unsloth/GLM-4-9B-0414-GGUF:Q8_K_XL
# 4. 启动应用: docker-compose up -d
# 5. 访问: http://localhost:8300

# 推荐方案 2：使用 DeepSeek（低成本 API）
# 1. 注册账号: https://platform.deepseek.com
# 2. 获取 API key
# 3. 设置环境变量: DEEPSEEK_API_KEY=your_key
# 4. 启动应用: docker-compose up -d
# 5. 访问: http://localhost:8300

# 推荐方案 3：使用 OpenAI（高性能）
# 1. 注册账号: https://platform.openai.com
# 2. 获取 API key
# 3. 设置环境变量: OPENAI_API_KEY=your_key
# 4. 启动应用: docker-compose up -d
# 5. 访问: http://localhost:8300
